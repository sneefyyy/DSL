{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eefbfa",
   "metadata": {},
   "source": [
    "# DSL — Colab finetune (PEFT + W&B)\n",
    "\n",
    "This notebook clones the repo, installs dependencies (including optional PEFT/LoRA), mounts Drive (optional), and launches a PEFT finetune that logs to Weights & Biases.\n",
    "\n",
    "\n",
    "Set Runtime → Change runtime type → GPU before running. Use Colab Pro if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94982a1e",
   "metadata": {},
   "source": [
    "## 1) Clone repo and install dependencies\n",
    "\n",
    "The cell below clones the repository and installs required packages. If `bitsandbytes` fails to install on your runtime, you can remove that line and continue without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo and install deps\n",
    "!git clone https://github.com/sneefyyy/DSL.git /content/DSL || (cd /content/DSL && git pull)\n",
    "%cd /content/DSL\n",
    "# Upgrade pip and install base requirements\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "# Optional but recommended for PEFT/LoRA and faster training\n",
    "!pip install -U accelerate peft\n",
    "# Optional: bitsandbytes for 8-bit optimizations (may require CUDA drivers). If it errors, re-run without this line.\n",
    "!pip install bitsandbytes || true\n",
    "# Install wandb for logging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3866d5",
   "metadata": {},
   "source": [
    "## 2) (optional) Mount Google Drive\n",
    "\n",
    "If you want to read/write checkpoints to Drive, mount it here. Otherwise checkpoints will be written to the Colab VM (ephemeral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0813fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: mount Google Drive\n",
    "mount_drive = False  # set True to mount if you want Drive persistence\n",
    "output_dir = '/content/dsl-finetuned'\n",
    "if mount_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    output_dir = '/content/drive/MyDrive/dsl-finetuned'\n",
    "print('output_dir =', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7699f7",
   "metadata": {},
   "source": [
    "## 3) Provide Hugging Face and W&B credentials (secure)\n",
    "\n",
    "Enter tokens below. These are stored in environment variables for the notebook session only (not saved to Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "hf_token = getpass('Hugging Face token (or Enter to skip): ')\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    # optional: login via huggingface-cli if available\n",
    "    try:\n",
    "        get_ipython().system_raw(f\"huggingface-cli login --token {hf_token}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "wandb_key = getpass('Weights & Biases API key (or Enter to skip): ')\n",
    "if wandb_key:\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    os.environ.setdefault('WANDB_MODE', 'online')\n",
    "print('HF_TOKEN set:', 'HF_TOKEN' in os.environ, 'WANDB set:', 'WANDB_API_KEY' in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c155636",
   "metadata": {},
   "source": [
    "## 4) Quick checks: Python, GPU, and installed packages\n",
    "\n",
    "Confirm GPU visibility and important packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8566f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys, os\n",
    "print('Python', sys.version)\n",
    "print('Torch', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "try:\n",
    "    import peft\n",
    "    print('peft OK')\n",
    "except Exception:\n",
    "    print('peft not installed or import failed')\n",
    "import wandb\n",
    "print('wandb mode =', os.environ.get('WANDB_MODE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97e791",
   "metadata": {},
   "source": [
    "## 5) Run the finetune (PEFT + W&B)\n",
    "\n",
    "Adjust model, dataset, and training args as needed. If you saved dataset locally to Drive, use that path for `--dataset_id`. The example below launches the script in the background; remove the ampersand and `&` to run in foreground and stream logs to the notebook output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d44979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training command and run\n",
    "model_name = 'gpt2'\n",
    "dataset_id = 'middles/dsl-arc-dataset-v0.0.1'  # replace with local path if needed\n",
    "output_dir = '/content/dsl-finetuned'\n",
    "use_peft = True\n",
    "use_wandb = True\n",
    "wandb_project = 'dsl-finetune'\n",
    "peft_r = 8\n",
    "peft_alpha = 32\n",
    "# Optionally run W&B offline instead of online:\n",
    "# os.environ['WANDB_MODE'] = 'offline'\n",
    "cmd = 'python3 src/dsl/finetune_hf.py '\n",
    "cmd += f\"--model_name_or_path {model_name} \"\n",
    "cmd += f\"--dataset_id {dataset_id} \"\n",
    "cmd += f\"--output_dir {output_dir} \"\n",
    "cmd += '--num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 '\n",
    "if use_wandb:\n",
    "    cmd += '--use_wandb '\n",
    "    cmd += f'--wandb_project {wandb_project} '\n",
    "if use_peft:\n",
    "    cmd += '--use_peft '\n",
    "    cmd += f'--peft_r {peft_r} --peft_alpha {peft_alpha} '\n",
    "print('Running finetune command:')\n",
    "print(cmd)\n",
    "# Start in background so the notebook remains responsive; remove '&' to run in foreground\n",
    "get_ipython().system_raw(cmd + ' &')\n",
    "print('Finetune started in background. Use `!ps aux | grep finetune_hf.py` to check, or inspect logs under the output dir.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa8647",
   "metadata": {},
   "source": [
    "## 6) View W&B runs and tables\n",
    "\n",
    "- Online: open the W&B project page at https://wandb.ai/<your-entity>/<project>\n",
    "- Offline: runs are stored locally under `./wandb/` and can be inspected in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show local wandb offline directories (if any)\n",
    "import os, glob, json\n",
    "for d in sorted(glob.glob('wandb/offline-run-*')):\n",
    "    print(d)\n",
    "    for f in os.listdir(d)[:50]:\n",
    "        print('  ', f)\n",
    "# Search for functional_predictions artifacts in offline runs\n",
    "for d in glob.glob('wandb/offline-run-*'):\n",
    "    for p in glob.glob(d + '/**/functional_predictions*', recursive=True):\n",
    "        print('Found table file:', p)\n",
    "        try:\n",
    "            with open(p, 'r') as fh:\n",
    "                for i, line in enumerate(fh):\n",
    "                    if i >= 5:\n",
    "                        break\n",
    "                    print('  ', line.strip())\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3de19",
   "metadata": {},
   "source": [
    "### Notes and tips\n",
    "- Replace `middles/dsl-arc-dataset-v0.0.1` with a local dataset path if you uploaded it to Drive.\n",
    "- Use smaller batch sizes on Colab if you run out of GPU memory.\n",
    "- To avoid W&B uploads during debugging, set `os.environ['WANDB_MODE']='offline'` before launching the training command.\n",
    "- If you want the training logs streamed live in the notebook, remove the ampersand `&` at the end of the command invocation in the training cell."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
